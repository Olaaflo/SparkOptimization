{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bc64627-1ee3-4906-8ebc-c87f330b745a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    FloatType,\n",
    "    BooleanType,\n",
    "    TimestampType,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ba6066e-d846-4248-a3d1-5dc3bbbcc39a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1. Drop the Existing Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00a6a2f6-065e-45e5-8ef8-6fe018a1e423",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped the tables\n"
     ]
    }
   ],
   "source": [
    "# To drop all tables and start over from eventlog files\n",
    "if False:\n",
    "    spark.sql(\"drop table eventlog_raw;\")\n",
    "    spark.sql(\"drop table queries;\")\n",
    "    spark.sql(\"drop table operations;\")\n",
    "    spark.sql(\"drop table physical_plan_keys;\")\n",
    "    print(\"Dropped the tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b574724b-cfe9-42a4-949a-1d04877dea98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped the tables\n"
     ]
    }
   ],
   "source": [
    "# To drop all tables and start over from eventlog files\n",
    "if False:\n",
    "    spark.sql(\"drop table method_runs;\")\n",
    "    spark.sql(\"drop table method_recommendations;\")\n",
    "    spark.sql(\"drop table method_results;\")\n",
    "    print(\"Dropped the tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ac82321-e631-4140-80f3-a39680945940",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2. Create New Tables\n",
    "* method_runs - table with metadata for each method\\_run\n",
    "  * **runId**: unique id for the method run\n",
    "  * **methodName**: which method used\n",
    "  * **params**: parameters used for the method\n",
    "  * **fromTime**: time from when the interval \"starts\"\n",
    "  * **toTime**: time from when the interval \"starts\"\n",
    "  * **whenRun**: time of when the method was ran\n",
    "* method_recommendations - table with column recommendations for each table in each method\\_run\n",
    "  * **runId**: same id as the method\\_runs table\n",
    "  * **tableName**: the table that is being recommendation on\n",
    "  * **columnName**: the column that is recommended to partition on based on the method run\n",
    "  * **isPartitioned**: true/false if the recommended column is already partitioned on or not\n",
    "* method_results - table with metrics for each column in each table for each method run\n",
    "  * **runId**: same id as the method\\_runs table\n",
    "  * **tableName**: table name of relevant table\n",
    "  * **columnName**: column name of relevant column\n",
    "  * **methodValue**: value associated with the specific method\n",
    "  * **isPartitioned**: true/false if the recommended column is already partitioned on or not\n",
    "\n",
    "\n",
    "\n",
    "```Note:``` the method_runs.**params** column will be used at the start, but if we notice that we need to have a column for eac parameter (to make it easier to filter on spesific parameters at a later stage, we can easily explode the json and create columns instead)\n",
    "\n",
    "```Note2:``` the multiple methods implemented must **NOT** be run in parallel when creating workflows. Reason being is that runId might get duplicated making it no long unique for each \"run\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17beb966-1d86-4f54-baa2-7009b2f4822a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE table IF NOT EXISTS method_runs (\n",
    "  runId INT,\n",
    "  methodName STRING,\n",
    "  params STRING,\n",
    "  --Store parameters as a JSON string\n",
    "  fromTime TIMESTAMP,\n",
    "  toTime TIMESTAMP,\n",
    "  whenRun TIMESTAMP\n",
    ");\n",
    "CREATE table IF NOT EXISTS method_recommendations (\n",
    "  runId INT,\n",
    "  databaseName STRING,\n",
    "  tableName STRING,\n",
    "  columnName STRING,\n",
    "  methodValue FLOAT,\n",
    "  isPartitioned BOOLEAN\n",
    ");\n",
    "CREATE table IF NOT EXISTS method_results (\n",
    "  runId INT,\n",
    "  databaseName STRING,\n",
    "  tableName STRING,\n",
    "  columnName STRING,\n",
    "  methodValue FLOAT,\n",
    "  isPartitioned BOOLEAN\n",
    ");"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1887537498594749,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "SetupTables",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
