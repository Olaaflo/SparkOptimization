{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c911b42-d695-42bc-a9c3-78ca942925d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Rule Based Recommendation Method - Simple Count\n",
    "  * A rule based method for partition recommendation\n",
    "  * Counts the number of times a column has been filtered on for each table.\n",
    "  * Recommend this as a new partition if this column has been filtered on more often than the current partition of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb4209f4-6a80-4a3b-8a52-07d2339d1c5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import json\n",
    "from datetime import timedelta, datetime\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    FloatType,\n",
    "    BooleanType,\n",
    "    TimestampType,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07eaf813-3465-4f57-b42f-36b84de74c44",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1. Inputs\n",
    "* **toTime**: (yyyy-mm-dd HH:MM:SS) from when you want to start the relevant interval (last date)\n",
    "* **interval**: (weeks) how many weeks (starting from toTime and goes backwards) you want to use data\n",
    "\n",
    "`Note: toTime is the END of the interval`\n",
    "\n",
    "`Note2: may include fromTime (either instead or as well as interval) as a input parameter if we find a purpose for it`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcc3f133-6341-496e-96ce-5e6e8c7f393a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Validators\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22d6b4d8-989f-4d86-a82f-6d6a0b27ac50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datetime_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "dbutils.widgets.text(\n",
    "    \"to_time\",\n",
    "    \"None\",\n",
    "    'toTime, End of interval, use \"None\" for time.now(), Format: yyyy-mm-dd HH:MM:SS',\n",
    ")\n",
    "\n",
    "dbutils.widgets.text(\n",
    "    \"interval\", \"4\", \"Size of interval (weeks) for how much data the methods will use\"\n",
    ")\n",
    "\n",
    "to_time = dbutils.widgets.get(\"to_time\")\n",
    "interval = dbutils.widgets.get(\"interval\")\n",
    "\n",
    "# Because workflows got no way of specifying time.now()\n",
    "if to_time == \"None\":\n",
    "    to_time = datetime.now()\n",
    "else:\n",
    "    # function from ./Validators notebook\n",
    "    if validate_time_input(to_time, datetime_format):\n",
    "        to_time = datetime.strptime(to_time, datetime_format)\n",
    "    else:\n",
    "        raise ValueError(\"inputted to_time is not on the right format\")\n",
    "\n",
    "assert validate_positive_number(interval), \"interval is not a valid number > 0\"\n",
    "interval = float(interval)\n",
    "\n",
    "from_time = to_time - timedelta(weeks=interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29a0ddcc-8cad-4771-a611-a1977fc41277",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Tables:\n",
    "* method_runs\n",
    "* method_results\n",
    "* method_recommendations\n",
    "\n",
    "Are already created in the **SetupTables** notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a8de287-d61b-4ed4-9d1c-fa7adec103bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "method_runs_schema = StructType(\n",
    "    [\n",
    "        StructField(\"runId\", IntegerType(), nullable=True),\n",
    "        StructField(\"methodName\", StringType(), nullable=True),\n",
    "        StructField(\"params\", StringType(), nullable=True),\n",
    "        StructField(\"fromTime\", TimestampType(), nullable=True),\n",
    "        StructField(\"toTime\", TimestampType(), nullable=True),\n",
    "        StructField(\"whenRun\", TimestampType(), nullable=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e54c6b31-c540-476b-abed-f94560b80249",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2. Preprocessing\n",
    "##### Fetch the data from the operations table and perform the following steps:\n",
    "* Only fetch operations from within the **from_time** and **to_time** input parameters\n",
    "* Ignore the \"Filter\" rows as columns in here seems to already be stored under the \"PushedFilters\" rows\n",
    "* Remove rows with NA values and empty strings (may have happended when extracting the table and database name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d8ae5b7-2e43-4c7f-ad79-79d791b7f61f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2721543019531124>:8\u001B[0m\n",
       "\u001B[1;32m      3\u001B[0m to_time_timestamp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(to_time\u001B[38;5;241m.\u001B[39mtimestamp() \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m)\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Filter data not between from and to-date\u001B[39;00m\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Remove rows where operation name isn't Filter (end up with PartitionFilters and PushedFilters)\u001B[39;00m\n",
       "\u001B[1;32m      7\u001B[0m operations \u001B[38;5;241m=\u001B[39m (\n",
       "\u001B[0;32m----> 8\u001B[0m     spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSELECT * FROM operations\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      9\u001B[0m     \u001B[38;5;241m.\u001B[39mfilter(F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeGenerated\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mbetween(from_time_timestamp, to_time_timestamp))\n",
       "\u001B[1;32m     10\u001B[0m     \u001B[38;5;241m.\u001B[39mfilter(F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moperationName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFilter\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     11\u001B[0m )\n",
       "\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Remove empty rows\u001B[39;00m\n",
       "\u001B[1;32m     14\u001B[0m operations \u001B[38;5;241m=\u001B[39m operations\u001B[38;5;241m.\u001B[39mselect(\n",
       "\u001B[1;32m     15\u001B[0m     [\n",
       "\u001B[1;32m     16\u001B[0m         F\u001B[38;5;241m.\u001B[39mwhen(operations[col] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\u001B[38;5;241m.\u001B[39motherwise(operations[col])\u001B[38;5;241m.\u001B[39malias(col)\n",
       "\u001B[1;32m     17\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m operations\u001B[38;5;241m.\u001B[39mcolumns\n",
       "\u001B[1;32m     18\u001B[0m     ]\n",
       "\u001B[1;32m     19\u001B[0m )\u001B[38;5;241m.\u001B[39mdropna(how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124many\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n",
       "\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `operations` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
       "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
       "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n",
       "'Project [*]\n",
       "+- 'UnresolvedRelation [operations], [], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2721543019531124>:8\u001B[0m\n\u001B[1;32m      3\u001B[0m to_time_timestamp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(to_time\u001B[38;5;241m.\u001B[39mtimestamp() \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Filter data not between from and to-date\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Remove rows where operation name isn't Filter (end up with PartitionFilters and PushedFilters)\u001B[39;00m\n\u001B[1;32m      7\u001B[0m operations \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m----> 8\u001B[0m     spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSELECT * FROM operations\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;241m.\u001B[39mfilter(F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeGenerated\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mbetween(from_time_timestamp, to_time_timestamp))\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;241m.\u001B[39mfilter(F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moperationName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFilter\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     11\u001B[0m )\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Remove empty rows\u001B[39;00m\n\u001B[1;32m     14\u001B[0m operations \u001B[38;5;241m=\u001B[39m operations\u001B[38;5;241m.\u001B[39mselect(\n\u001B[1;32m     15\u001B[0m     [\n\u001B[1;32m     16\u001B[0m         F\u001B[38;5;241m.\u001B[39mwhen(operations[col] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\u001B[38;5;241m.\u001B[39motherwise(operations[col])\u001B[38;5;241m.\u001B[39malias(col)\n\u001B[1;32m     17\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m operations\u001B[38;5;241m.\u001B[39mcolumns\n\u001B[1;32m     18\u001B[0m     ]\n\u001B[1;32m     19\u001B[0m )\u001B[38;5;241m.\u001B[39mdropna(how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124many\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `operations` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [operations], [], false\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `operations` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [operations], [], false\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert from and to-time to the same format as the table\n",
    "from_time_timestamp = int(from_time.timestamp() * 1000)\n",
    "to_time_timestamp = int(to_time.timestamp() * 1000)\n",
    "\n",
    "# Filter data not between from and to-date\n",
    "# Remove rows where operation name isn't Filter (end up with PartitionFilters and PushedFilters)\n",
    "operations = (\n",
    "    spark.sql(\"SELECT * FROM operations\")\n",
    "    .filter(F.col(\"timeGenerated\").between(from_time_timestamp, to_time_timestamp))\n",
    "    .filter(F.col(\"operationName\") != \"Filter\")\n",
    ")\n",
    "\n",
    "# Remove empty rows\n",
    "operations = operations.select(\n",
    "    [\n",
    "        F.when(operations[col] == \"\", None).otherwise(operations[col]).alias(col)\n",
    "        for col in operations.columns\n",
    "    ]\n",
    ").dropna(how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "116818dd-5474-4560-bc9d-506107b0a0bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This code is responsible for creating a dataframe where you can check whether a table is partitioned on a particular column\n",
    "# Used as a lookup before writing results to tables after having run some of the methods\n",
    "\n",
    "# Group by databaseName, tableName, columnName and find the max timeGenerated\n",
    "max_time_df = operations.groupBy(\"databaseName\", \"tableName\", \"columnName\").agg(\n",
    "    F.max(\"timeGenerated\").alias(\"maxTimeGenerated\")\n",
    ")\n",
    "\n",
    "\n",
    "# Join the max_time_df with the original dataframe\n",
    "max_time_df = (\n",
    "    operations.join(max_time_df, [\"databaseName\", \"tableName\", \"columnName\"])\n",
    "    .filter(F.col(\"timeGenerated\") == F.col(\"maxTimeGenerated\"))\n",
    "    .groupBy(\"databaseName\", \"tableName\", \"columnName\")\n",
    "    .agg(\n",
    "        F.first(F.col(\"operationName\")).alias(\"operationName\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "is_partitioned = max_time_df.withColumn(\n",
    "    \"isPartitioned\",\n",
    "    F.when(F.col(\"operationName\") == \"PartitionFilters\", True).otherwise(False),\n",
    ").drop(\"operationName\")\n",
    "\n",
    "\n",
    "display(is_partitioned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94102390-dded-4caa-ad55-507401fc305f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3. Method - count\n",
    "For each databse, table, column; check how often the column is used for filtering (PartitionFilters, Filter), count the occurences\n",
    "##### parameters:\n",
    "* **windowStart**: (int - unix_ms) window start of which data to be used\n",
    "* **windowEnd**: (int - unix_ms) window end of which data to be used\n",
    "* **windowSize**: (float) number of weeks of which the interval spans\n",
    "\n",
    "\n",
    "##### metadata of method:\n",
    "* **whenRun**: (timestamp) when the method is ran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68a6f1f5-b0e4-49ef-ade2-dae2b27e37c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "runId = 1\n",
    "# update runId if there is already a max_id in the method_runs table\n",
    "max_id = spark.sql(\"SELECT MAX(runId) AS max_id FROM method_runs\").collect()[0][\n",
    "    \"max_id\"\n",
    "]\n",
    "if max_id is not None:\n",
    "    runId = max_id + 1\n",
    "print(f\"runId: {runId}\")\n",
    "\n",
    "params = {\n",
    "    \"windowStart\": from_time_timestamp,\n",
    "    \"windowEnd\": to_time_timestamp,\n",
    "    \"windowSize\": interval,\n",
    "}\n",
    "metadata = {\n",
    "    \"whenRun\": datetime.now(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6b42189-a252-455c-955c-96f2eca7ae65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "method_run_info = {\n",
    "    \"runId\": runId,\n",
    "    \"methodName\": \"simpleCount\",\n",
    "    \"params\": json.dumps(params) if params else \"\",\n",
    "    \"fromTime\": from_time,\n",
    "    \"toTime\": to_time,\n",
    "    \"whenRun\": metadata[\"whenRun\"],\n",
    "}\n",
    "\n",
    "method_run = spark.createDataFrame([method_run_info], schema=method_runs_schema)\n",
    "display(method_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e503d6e3-1b4c-4567-9403-6af828a36b62",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3.1 Count the Number of times each column is used for filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8740412-69d5-4276-9acb-fa882796a886",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "method_output = (\n",
    "    operations.groupBy(\"databaseName\", \"tableName\", \"columnName\")\n",
    "    .agg(F.count(\"executionId\").alias(\"occurrences\"))\n",
    "    .orderBy(\"databaseName\", \"tableName\", \"columnName\")\n",
    ")\n",
    "\n",
    "method_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d23cc1a-34eb-4d3e-b66c-94e165193ff2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3.2 Create Columns Needed for the method_results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "008ea204-cbda-482c-a204-d63e08c397ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# add needed columns to method_results table\n",
    "method_results = (\n",
    "    method_output.withColumn(\"methodValue\", F.col(\"occurrences\").cast(\"float\"))\n",
    ").drop(\"occurrences\")\n",
    "\n",
    "# join on is_partitioned dataframe to check if columns is partitioned or not\n",
    "# note: this is based on the last occurence of the filter in the operations table\n",
    "method_results = method_results.join(\n",
    "    is_partitioned, on=[\"databaseName\", \"tableName\", \"columnName\"]\n",
    ")\n",
    "\n",
    "method_results = method_results.withColumn(\"runId\", F.lit(runId)).select(\n",
    "    \"databaseName\",\n",
    "    \"tableName\",\n",
    "    \"columnName\",\n",
    "    \"methodValue\",\n",
    "    \"isPartitioned\",\n",
    "    \"runId\",\n",
    ")\n",
    "\n",
    "method_results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "732d323d-d053-461b-9251-f22eeae2c62f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3.3 Find Recommendations Based on the Column with the Highest methodValue per db-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9358346d-2659-4c29-b385-b6861a1a4079",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "method_recommendations = (\n",
    "    method_results.groupBy(\"databaseName\", \"tableName\")\n",
    "    .agg(\n",
    "        F.max(F.struct(\"methodValue\", \"columnName\", \"isPartitioned\")).alias(\n",
    "            \"max_methodValue_colName_isPartitioned\"\n",
    "        )\n",
    "    )\n",
    "    .select(\n",
    "        \"databaseName\",\n",
    "        \"tableName\",\n",
    "        \"max_methodValue_colName_isPartitioned.columnName\",\n",
    "        \"max_methodValue_colName_isPartitioned.methodValue\",\n",
    "        \"max_methodValue_colName_isPartitioned.isPartitioned\",\n",
    "    )\n",
    "    .withColumn(\"runId\", F.lit(runId))\n",
    "    .withColumnRenamed(\"max_methodValue_colName_isPartitioned.columnName\", \"columnName\")\n",
    "    .withColumnRenamed(\n",
    "        \"max_methodValue_colName_isPartitioned.methodValue\", \"methodValue\"\n",
    "    )\n",
    "    .withColumnRenamed(\n",
    "        \"max_methodValue_colName_isPartitioned.isPartitioned\", \"isPartitioned\"\n",
    "    )\n",
    ")\n",
    "\n",
    "method_recommendations.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "754c82a7-df51-4862-859a-788c216a6858",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 4. Save data to tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c5fd63b-b873-4b06-8a87-1451b67de0c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### 6. Save data to tables\n",
    "\n",
    "# write method_runs information\n",
    "method_run.write.format(\"delta\").mode(\"append\").saveAsTable(\"method_runs\")\n",
    "\n",
    "# write method_results information\n",
    "method_results.write.format(\"delta\").mode(\"append\").saveAsTable(\"method_results\")\n",
    "\n",
    "# write method_recommendation information\n",
    "method_recommendations.write.format(\"delta\").mode(\"append\").saveAsTable(\n",
    "    \"method_recommendations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb8d1492-1fd0-40d0-ab49-b6c6b85811d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select\n",
    "  *\n",
    "from\n",
    "  method_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c1634fe-9114-4172-a1de-71839a24b891",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select\n",
    "  *\n",
    "from\n",
    "  method_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8dfe0cb-47ef-47fa-bbbf-a7e42cc9da92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select\n",
    "  *\n",
    "from\n",
    "  method_recommendations"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2721543019531134,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4,
    "widgetLayout": [
     {
      "breakBefore": false,
      "name": "from_time",
      "width": 285
     },
     {
      "breakBefore": false,
      "name": "to_time",
      "width": 571
     },
     {
      "breakBefore": false,
      "name": "interval",
      "width": 428
     }
    ]
   },
   "notebookName": "RuleBased_count",
   "widgets": {
    "interval": {
     "currentValue": "16",
     "nuid": "2a9ec364-2504-496c-b642-e2b91c2e3ab9",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "4",
      "label": "Size of interval (weeks) for how much data the methods will use",
      "name": "interval",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    },
    "to_time": {
     "currentValue": "None",
     "nuid": "165ce4ba-be00-4826-920c-d838b8bdc390",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "None",
      "label": "toTime, End of interval, use \"None\" for time.now(), Format: yyyy-mm-dd HH:MM:SS",
      "name": "to_time",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
