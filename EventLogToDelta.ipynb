{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af8e4c76-548e-4fa2-9751-72aea42c8f04",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Event Log To Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a09c0df-a5b9-4498-9292-cd23332c77ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import gzip\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    TimestampType,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d07f751-9f41-4930-9af0-d374c61c29ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1. Inputs  \n",
    "- start_time: Timestamp default: Two weeks prior\n",
    "- end_time: Timestamp default: time_now()\n",
    "- path_to_cluster_logs: Root logging path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "510d46da-21df-452c-9a96-3852bbf4c504",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"start_time\", \"None\", \"StartTime, Format: Y-%m-%d %H:%M:%S\")\n",
    "dbutils.widgets.text(\"end_time\", \"None\", \"EndTime, Format: Y-%m-%d %H:%M:%S\")\n",
    "dbutils.widgets.text(\n",
    "    \"path_to_cluster_logs\", \"/dbfs/cluster-logs\", \"Path to cluster logs\"\n",
    ")\n",
    "\n",
    "\n",
    "# get variables\n",
    "start_time = dbutils.widgets.get(\"start_time\")\n",
    "end_time = dbutils.widgets.get(\"end_time\")\n",
    "path_to_cluster_logs = dbutils.widgets.get(\"path_to_cluster_logs\")\n",
    "\n",
    "# Because of work flows\n",
    "time_now = datetime.now()\n",
    "\n",
    "# Handle start_time conversion\n",
    "if start_time == \"None\":\n",
    "    # 2 weeks prior\n",
    "    start_time = time_now - timedelta(days=14)\n",
    "else:\n",
    "    try:\n",
    "        start_time = datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')\n",
    "    except ValueError:\n",
    "        raise ValueError('inputted start_time is not on the right format')\n",
    "\n",
    "# Handle end_time conversion\n",
    "if end_time == \"None\":\n",
    "    end_time = time_now\n",
    "else:\n",
    "    try:\n",
    "        end_time = datetime.strptime(end_time, '%Y-%m-%d %H:%M:%S')\n",
    "    except ValueError:\n",
    "        raise ValueError('inputted end_time is not on the right format')\n",
    "\n",
    "\n",
    "assert start_time < end_time, \"Start time needs to be before end time.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fb93d41-b5bf-4caf-ab6c-d0fd5a9873d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2. Load Data From File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f3a0e3b-3fa5-4b19-a3b8-a7431e7bbc66",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2.1 Extract Metadata From Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d0435aa-e366-4317-b013-13521c57e7b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pathlist = Path(path_to_cluster_logs).glob(\"**/eventlog/**/eventlog*\")\n",
    "\n",
    "\n",
    "def find_timestamp(file_name):\n",
    "    match = re.match(r\"^eventlog-(\\d{4}-\\d{2}-\\d{2}--\\d{2}-\\d{2})\\.gz$\", file_name)\n",
    "    if match:\n",
    "        datetime_str = match.group(1)\n",
    "        dt = datetime.strptime(datetime_str, \"%Y-%m-%d--%H-%M\")\n",
    "        return dt\n",
    "\n",
    "\n",
    "file_metadata = []\n",
    "for path in pathlist:\n",
    "    path_string = str(path)\n",
    "    is_zipped = Path(path_string).suffix == \".gz\"\n",
    "    f = path_string.split(\"/\")\n",
    "\n",
    "    # if filename is 'eventlog', file is not zipped and stored with a timestamp\n",
    "    if f[7] == \"eventlog\":\n",
    "        unix_time = Path(path_string).stat().st_mtime\n",
    "        dt = datetime.fromtimestamp(unix_time)\n",
    "\n",
    "    # file is zipped and stored with a timestamp, finds the timestmap with regex\n",
    "    else:\n",
    "        timestamp = f[7].split(\"eventlog\")\n",
    "        dt = find_timestamp(f[7])\n",
    "\n",
    "    # metadata consisting of\n",
    "    # (path_object, cluster_id, cluster_instance_id, spark_context_id, last_modification_timestamp, bool_file_is_zipped)\n",
    "    metadata = (path, f[3], f[5], f[6], dt, is_zipped)\n",
    "\n",
    "    file_metadata.append(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84f1c115-90c3-4346-aa75-cc5a9df020f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2.2 Filter On Input Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6980f77e-985f-46f6-8049-6a2bbcf01a3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: (datetime.datetime(2023, 6, 1, 9, 0), datetime.datetime(2023, 6, 1, 12, 0))"
     ]
    }
   ],
   "source": [
    "start_time, end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc42d989-fa68-48b7-aa87-f51bab954f3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in time interval: 3\n"
     ]
    }
   ],
   "source": [
    "relevant_files = [\n",
    "    file_meta for file_meta in file_metadata if start_time < file_meta[4] < end_time\n",
    "]\n",
    "print(f\"Number of files in time interval: {len(relevant_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14b3b0c5-019b-4953-9688-e9d8b88a0323",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: [datetime.datetime(2023, 6, 1, 10, 5, 16),\n datetime.datetime(2023, 6, 1, 9, 45),\n datetime.datetime(2023, 6, 1, 10, 0)]"
     ]
    }
   ],
   "source": [
    "# Print dates for each file\n",
    "print(len(relevant_files), 'relevant files')\n",
    "[f[4] for f in relevant_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "901858b3-0020-4618-bfa5-85b7c3cf7546",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2.3 Load Data From Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3265cb7-a2c0-42e1-b659-5e7e0a17a10a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for file_meta in relevant_files:\n",
    "    (\n",
    "        path_object,\n",
    "        cluster_id,\n",
    "        cluster_instance_id,\n",
    "        spark_context_id,\n",
    "        last_modification_timestamp,\n",
    "        is_zipped,\n",
    "    ) = file_meta\n",
    "\n",
    "    # string content of file\n",
    "    file_data = \"\"\n",
    "    if is_zipped:\n",
    "        with gzip.open(str(path_object), \"rb\") as g:\n",
    "            file_data = g.read().decode(\"UTF-8\")\n",
    "    else:\n",
    "        with open(str(path_object), \"r\") as f:\n",
    "            file_data = f.read()\n",
    "\n",
    "    row = (\n",
    "        str(path_object),\n",
    "        cluster_id,\n",
    "        cluster_instance_id,\n",
    "        spark_context_id,\n",
    "        last_modification_timestamp,\n",
    "        file_data,\n",
    "    )\n",
    "    data.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "487ad532-39e2-451f-ad07-cf76de080211",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2.4 Handle large eventlogs\n",
    "If eventlogs are above the size 268_435_456 bytes, it validates spark.rpc.message.maxSize.\n",
    "We therefore split eventlogs above 200_000_000 bytes into half, to make sure data is on a manageable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d21f796-4679-452e-b43c-b12c1208b4f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def split_eventlog(eventlog):\n",
    "    events = eventlog[-1].split('\\n')\n",
    "\n",
    "    if len(events) < 2:\n",
    "        raise Exception(f\"One of the eventlogs you are trying to split has a max size above the input given. But has fever than 2 events, thus can not be splitted further\")\n",
    "\n",
    "    midpoint = len(events) // 2\n",
    "\n",
    "    first_half = list(eventlog)\n",
    "    first_half[-1] = \"\\n\".join(events[:midpoint])\n",
    "\n",
    "    last_half = list(eventlog)\n",
    "    last_half[-1] = \"\\n\".join(events[midpoint:])\n",
    "\n",
    "    return first_half, last_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75dfc46c-3910-490f-a848-be596606d799",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# will abort if a file about this threshold is found\n",
    "max_size = 6.4 * 10**9  # 6.4 GB\n",
    "\n",
    "# max size of 200MB per eventlog\n",
    "max_desired_size = 0.2 * 10**9  # 0.2 GB\n",
    "\n",
    "sizes = [len(d[-1]) for d in data]\n",
    "if any([s > max_size for s in sizes]):\n",
    "    raise Exception(f\"Found eventlog-file > {max_size/10**9}GB. Aborting\")\n",
    "\n",
    "n = 5\n",
    "# n=1 allows for files of 400MB to pass\n",
    "# n=2 allows for files of 800MB to pass\n",
    "# n=3 allows for files of 1.6GB to pass\n",
    "# n=4 allows for files of 3.2GB to pass\n",
    "# n=5 allows for files of 6.4GB to pass\n",
    "for _ in range(n):\n",
    "    splitted_data = []\n",
    "    for d in data:\n",
    "        if len(d[-1]) > max_desired_size:\n",
    "            first_half, last_half = split_eventlog(d)\n",
    "            splitted_data.append(first_half)\n",
    "            splitted_data.append(last_half)\n",
    "        else:\n",
    "            splitted_data.append(d)\n",
    "\n",
    "    data = splitted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a591f61d-682c-46d0-ac74-a2d2b34f02b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3. Write Data To Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10330cd8-43d7-43e3-aa8b-b08dfa106fb1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Specify schema\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"filePath\", StringType(), True),\n",
    "        StructField(\"clusterID\", StringType(), True),\n",
    "        StructField(\"clusterInstanceID\", StringType(), True),\n",
    "        StructField(\"sparkContextID\", StringType(), True),\n",
    "        StructField(\"lastModified\", TimestampType(), True),\n",
    "        StructField(\"fileData\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "new_eventlogs = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "new_eventlogs.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3952f94-0f43-483c-924a-5158b23ac887",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Create storage table if it doesn't exist already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37b7f273-6841-4031-9562-f937b13a8c76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS eventlog_raw (\n",
    "  filePath STRING,\n",
    "  clusterID STRING,\n",
    "  clusterInstanceID STRING,\n",
    "  sparkContextID STRING,\n",
    "  lastModified TIMESTAMP,\n",
    "  fileData STRING,\n",
    "  eventlogKey INT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d39dcd08-3c43-41c8-b011-6d06c55a4f0e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Create a unique id for each of the eventlogs\n",
    "* combine the cluster instance, spark context and lastmodified strings, and use sparks hashing algorithm to create a hash\n",
    "* creating this key to prevent duplicate storage of eventlogs in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1564e84-8d75-4680-aa4a-d4223b5b06a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a unique key: eventlogKey\n",
    "new_eventlogs = new_eventlogs.withColumn(\n",
    "    \"eventlogKey\",\n",
    "    F.hash(F.concat(\"clusterInstanceID\", \"sparkContextID\", \"lastModified\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b680a5cc-6c6d-41bc-9426-3dd3bd43d5cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load eventlogKeys from existing data\n",
    "existing_keys = spark.sql(\"select eventlogKey from eventlog_raw\")\n",
    "\n",
    "# Perform anti-join on eventlogKeys to make sure the new data isnt a duplicate of what is already stored in the table\n",
    "new_eventlogs = new_eventlogs.join(existing_keys, \"eventlogKey\", \"anti\")\n",
    "\n",
    "# Nice to know that this number includes the splitting of eventlogs\n",
    "# meaning, one large eventlog counts as 2..\n",
    "print(f\"Adding {new_eventlogs.count()} new eventlogs to the table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d89caa7c-a01c-43d3-b76a-4a4e72d1e8a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write data to delta table\n",
    "delta_table_path = \"dbfs:/mnt/lake/\"\n",
    "new_eventlogs.write.format(\"delta\").mode(\"append\").saveAsTable(\"eventlog_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "410e286a-65d4-4f35-af65-88cc749335f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  COUNT(*)\n",
    "FROM\n",
    "  eventlog_raw;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4454456420357133,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4,
    "widgetLayout": [
     {
      "breakBefore": false,
      "name": "start_time",
      "width": 198
     },
     {
      "breakBefore": false,
      "name": "end_time",
      "width": 198
     },
     {
      "breakBefore": false,
      "name": "path_to_cluster_logs",
      "width": 198
     }
    ]
   },
   "notebookName": "EventLogToDelta",
   "widgets": {
    "end_time": {
     "currentValue": "2023-06-01 12:00:00",
     "nuid": "914c9c21-9486-4fb3-b61e-f3c7d3634864",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "None",
      "label": "EndTime, Format: Y-%m-%d %H:%M:%S",
      "name": "end_time",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    },
    "path_to_cluster_logs": {
     "currentValue": "/dbfs/cluster-logs",
     "nuid": "4c7f865e-b9ca-436e-ba26-4dd017a8dc65",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/dbfs/cluster-logs",
      "label": "Path to cluster logs",
      "name": "path_to_cluster_logs",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    },
    "start_time": {
     "currentValue": "2023-06-01 09:00:00",
     "nuid": "bca42c8b-f742-40e6-a73a-c8722a377a27",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "None",
      "label": "StartTime, Format: Y-%m-%d %H:%M:%S",
      "name": "start_time",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
